{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "923c9266",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Cross-validation\n",
    "\n",
    "With the code developed so far, it is possible to train an ANN and provide an estimate of the results it would offer in its real execution (with unseen patterns, represented by a test set). However, in this last aspect there are two factors to consider, as a consequence of the non-deterministic nature of the process we are following:\n",
    "\n",
    "- The partitioning of the set of patterns into training/test is random (hold out), and is therefore overly dependent on good or bad luck in choosing training and test patterns.\n",
    "- ANN training is not deterministic, as the initialisation of the weights is random. As before, it is too dependent on good or bad luck to start the training at a good or bad starting point.\n",
    "\n",
    "For these two reasons, the test result of a single training is not significant when assessing the goodness of fit of the model in the presence of unseen patterns. To solve this problem, the experiment is repeated several times and the results are averaged. This can be implemented in a simple way by means of a loop; however, it is necessary to do this in an orderly way as there are two different sources of randomness.\n",
    "\n",
    "Firstly, to minimise the randomness due to the partitioning of the data set, it is necessary to have a method that ensures that each data is used for training at least once, and for testing at least once. The most commonly used method is cross-validation. In this method, the data set is split into k disjoint subsets and k experiments are performed. In the k-th experiment, the k subset is separated for testing, and the remaining k-1 substes are used for training, performing a k-fold cross-validation. A common value is k=10, which gives a 10-fold cross-validation. Finally, the test value corresponding to the appropriate metric will be the average value of the values of the k experiments.\n",
    "\n",
    "A widely used variant of cross-validation is stratified cross-validation. In this case, each subset is created in such a way as to keep the proportion of patterns of each class the same (or similar) as in the original dataset. This is particularly used when the data set is imbalanced.\n",
    "\n",
    "It is usual to save not only the mean, but also the k values, in order to subsequently perform a paired hypothesis test with another model. To do this, it is necessary that both models have been trained using the same training and test sets.\n",
    "\n",
    "This way of evaluating the model is often considered to be slightly pessimistic, i.e. the results obtained in tests are slightly worse than those that would be obtained from real training with all available data. In a hold out experiment, as mentioned above, several data are separated for testing. This means that the model is trained with less data than is available, and that by chance the data separated for testing can be of great importance (especially if there is little data). For this reason, when training with less data and possibly no \"important\" data, hold out is considered a pessimistic assessment. In the same way, cross-validation also separates data for testing, so it does not train on all available data, and is therefore also pessimistic. However, it is guaranteed that all data are used at least once in training and once in testing, thus trying to minimise the impact of chance in separating data, so it is considered only a slightly pessimistic evaluation.\n",
    "\n",
    "Doing this is as simple as splitting the data set and performing a loop with k iterations in which at the k-th iteration a model is trained and evaluated with the corresponding sets. However, if the model is not deterministic, the result obtained at the k-th iteration will not be meaningful, since it is again dependent on chance. In this case, what needs to be done is a second nested loop within iteration k in which the model is repeatedly trained, and finally an average of the results is made to finally output the result of iteration k. The number of trainings must be high for the average results to be really significant, at least 50 trainings.\n",
    "\n",
    "### Question\n",
    "\n",
    "If this second loop is performed with a deterministic model, what will be the standard deviation of the test results obtained? Is there a difference between performing this second loop and averaging the results, or doing a single training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0294d11a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`If the second loop is performed with a deterministic model, the standard deviation of the test results obtained would be zero. This is because, by definition, a deterministic model will always produce the same result given the same training and test data, which means that each repeated training will yield identical test results. In conclusion, with a deterministic model, there is no difference between performing repeated training and averaging the results or doing a single training.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af15c6a2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this way, it is possible to evaluate a model together with its hyperparameters in solving a problem. A very common situation is to compare several models (or the same model with different hyperparameters), for which this scheme has to be applied with an important caveat: the sets used in the cross-validation must be the same for each model. Since the distribution of patterns in different sets is random, having the same subsets in different runs is achieved by setting the random seed at the beginning of the program to be executed. Setting the random seed not only allows the same subsets to be generated, but is also important in order to be able to repeat the results in different runs.\n",
    "\n",
    "It is also important to bear in mind that this methodology allows estimating the real performance of a model (although slightly pessimistic). The final model that would be used in production would be the result of training it with all the available patterns, since, as seen in the theory class, and very generally speaking, the more patterns you train with, the better the model will be.\n",
    "\n",
    "In this assignment, you are asked to:\n",
    "\n",
    "1. Develop a function called `crossvalidation` that receives a value `N` (equal to the number of patterns), and a value `k` (number of subsets into which the dataset is to be split), and returns a vector of length N, where each element indicates in which subset that pattern should be included.\n",
    "\n",
    "    To do this function, one possibility is to perform the following steps:\n",
    "    \n",
    "    - Create a vetor with k sorted elements, from 1 to k.\n",
    "    - Create a new vector with repetitions of the previous vector until its length is greater than or equal to N. The functions `repeat` and `ceil` can be used for this purpose.\n",
    "    - Take the first N values of this vector.\n",
    "    - Shuffle this vector (using the function `shuffle!` and return it. To use this function, the module `Random` should be loaded.\n",
    "    \n",
    "    No loop function should be used in the developed function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35def6db",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossvalidation (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Random\n",
    "\n",
    "function crossvalidation(N::Int64, k::Int64)\n",
    "    subsets = collect(1:k)\n",
    "    \n",
    "    num_repeats = ceil(Int, N / k)\n",
    "    extended_subsets = repeat(subsets, num_repeats)\n",
    "    \n",
    "    selected_subsets = extended_subsets[1:N]\n",
    "    \n",
    "    shuffle!(selected_subsets)\n",
    "    \n",
    "    return selected_subsets\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66eff66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running crossvalidation(N=12, k=4)...\n",
      "Result of crossvalidation:\n",
      "[4, 4, 3, 1, 2, 3, 3, 1, 2, 4, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# Example execution\n",
    "N = 12  # Number of patterns\n",
    "k = 4   # Number of subsets\n",
    "\n",
    "println(\"Running crossvalidation(N=$N, k=$k)...\")\n",
    "result = crossvalidation(N, k)\n",
    "\n",
    "println(\"Result of crossvalidation:\")\n",
    "println(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee3465",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Create a new function called `crossvalidation`, which in this case receives as first argument `targets` of type `AbstractArray{Bool,2}` with the desired outputs, and as second argument a value `k` (number of subsets in which the dataset will be split), and returns a vector of length N (equal to the number of rows of targets), where each element indicates in which subset that pattern must be included. This partition has also to be stratified. To do this, the following steps can be followed:\n",
    "\n",
    "    - Create a vector of indices, with as many values as rows in the `target` matrix.\n",
    "    - Write a loop that iterates over the classes (columns in the `target` matrix), and does the following:\n",
    "        - Take the number of elements belonging to that class. This can be done by making a call to the `sum` function applied to the corresponding column.\n",
    "        - Make a call to the `crossvalidation` function developed earlier passing as parameters this number of elements and the value of k.\n",
    "        - Update the index vector positions indicated by the corresponding column of the `targets` matrix with the values of the vector resulting from the call to the `crossvalidation` function.\n",
    "        \n",
    "        ### Question\n",
    "        \n",
    "        Could you perform these 3 operations in a single line of code?\n",
    "        \n",
    "        ```index_vector[findall(x -> x == true, targets[:, class])] .= crossvalidation(sum(targets[:, class]), k) ```\n",
    "        `sum(targets[:,class]) computes the count of patterns belonging to that class`\n",
    "        `crossvalidation call generates the indices for the class`\n",
    "        `index_vector[findall(x -> x == true, targets[:, class])] updates the positions in index_vector that correspond to the indices of the current class.`\n",
    "        </br>\n",
    "    - Return the vector of indices.\n",
    "    \n",
    "    As it can be seen in this explanation, a loop iterating all classes can be used in this function. However, you need to make sure that each class has at least k patterns. A usual value is k=10. Therefore, it is important to make sure that you have at least 10 patterns of each class.\n",
    "        \n",
    "    ### Question\n",
    "    \n",
    "    What would happen if any class has a number of patterns less than k? What would be the consequences for calculating metrics?\n",
    "    \n",
    "    ```Answer here```\n",
    "    \n",
    "    > If, for whatever reason, it is impossible to ensure that you have at least 10 patterns of each class, one possibility would be to lower the value of k. In this case, consult with the teacher to assess this option, and what impact it might have on the final result of the trained models. In this case, consult with the teacher to assess this option, and what impact it might have on the final result of the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5491483d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossvalidation (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function crossvalidation(targets::AbstractArray{Bool, 2}, k::Int64)\n",
    "    @assert k>1\n",
    "    N = size(targets, 1)  # Number of patterns (rows)\n",
    "    num_classes = size(targets, 2)  # Number of classes (columns)\n",
    "    \n",
    "    index_vector = zeros(Int, N)  # Create a vector to store the subset indices\n",
    "    \n",
    "    for class in 1:num_classes\n",
    "        index_vector[findall(x -> x == true, targets[:, class])] .= crossvalidation(sum(targets[:, class]), k)\n",
    "    end\n",
    "    \n",
    "    return index_vector\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f230967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.add(\"StatsBase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4810870",
   "metadata": {},
   "source": [
    "##### TEST FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c250e096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets (first 10 rows):\n",
      "Bool[1 1 0; 1 0 0; 1 0 0; 1 0 1; 0 1 0; 0 0 1; 1 0 1; 0 0 0; 1 0 1; 0 1 0]\n",
      "\n",
      "Indices for cross-validation (first 10):\n",
      "[5, 10, 2, 2, 6, 1, 1, 0, 9, 6]\n"
     ]
    }
   ],
   "source": [
    "using StatsBase\n",
    "function create_synthetic_data(n_samples::Int, n_classes::Int)\n",
    "    targets = rand(Bool, n_samples, n_classes) \n",
    "    return targets\n",
    "end\n",
    "\n",
    "n_samples = 100  \n",
    "n_classes = 3    \n",
    "\n",
    "targets = create_synthetic_data(n_samples, n_classes)\n",
    "\n",
    "println(\"Targets (first 10 rows):\")\n",
    "println(targets[1:10, :])\n",
    "\n",
    "k = 10  \n",
    "indices = crossvalidation(targets, k)\n",
    "\n",
    "println(\"\\nIndices for cross-validation (first 10):\")\n",
    "println(indices[1:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc74a4b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Perform a final function called crossvalidation, but in this case with the first parameter `targets` of type `AbstractArray{<:Any,1}` (i.e. a vector with heterogeneous elements), the same second argument, and perform stratified cross-validation.\n",
    "\n",
    "    In this case, the steps to follow in this function are not specified. However, they are similar to the previous one. A simple way to do it would be to call the function `oneHotEncoding` passing the vector `targets` as an argument.\n",
    "    \n",
    "      ### Question\n",
    "      \n",
    "      Could you develop this function without calling oneHotEncoding?\n",
    "      \n",
    "      ```Answer here```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbc619c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crossvalidation (generic function with 3 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function crossvalidation(targets::AbstractArray{<:Any, 1}, k::Int64)\n",
    "    unique_classes = unique(targets) \n",
    "    num_classes = length(unique_classes) \n",
    "    N = length(targets)\n",
    "    \n",
    "    binary_matrix = zeros(Bool, N, num_classes)  \n",
    "    \n",
    "    for (i, target) in enumerate(targets)\n",
    "        binary_matrix[i, findfirst(x -> x == target, unique_classes)] = true\n",
    "    end\n",
    "\n",
    "    index_vector = zeros(Int, N) \n",
    "    \n",
    "    for class in 1:num_classes\n",
    "        class_indices = findall(x -> x == true, binary_matrix[:, class])  \n",
    "        class_count = length(class_indices) \n",
    "        \n",
    "        if class_count >= k\n",
    "            index_vector[class_indices] .= crossvalidation(class_count, k)\n",
    "        else\n",
    "            println(\"Warning: Class $class has only $class_count instances, which is less than k ($k).\")\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return index_vector\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ac3c376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets:\n",
      "[\"cat\", \"dog\", \"horse\", \"horse\", \"cat\", \"horse\", \"cat\", \"dog\", \"horse\", \"cat\"]\n",
      "Warning: Class 2 has only 2 instances, which is less than k (3).\n",
      "\n",
      "Indices for cross-validation:\n",
      "[1, 0, 1, 2, 3, 1, 2, 0, 3, 1]\n",
      "\n",
      "Distribution of indices:\n",
      "Dict(0 => 2, 2 => 2, 3 => 2, 1 => 4)\n"
     ]
    }
   ],
   "source": [
    "targets = [\"cat\", \"dog\", \"horse\", \"horse\", \"cat\", \n",
    "\"horse\", \"cat\", \"dog\", \"horse\", \"cat\"]  \n",
    "    k = 3 \n",
    "\n",
    "    println(\"Targets:\")\n",
    "    println(targets)\n",
    "\n",
    "    indices = crossvalidation(targets, k)\n",
    "    \n",
    "    println(\"\\nIndices for cross-validation:\")\n",
    "    println(indices)\n",
    "    \n",
    "    println(\"\\nDistribution of indices:\")\n",
    "    println(countmap(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ddb115",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4. Integrate these functions into the code developed so far and define two functions to train ANNs following the stratified cross-validation strategy. To do this:\n",
    "\n",
    "- First, it is necessary to set the random seed to ensure that the experiments are repeatable. This can be done with the `seed!` function of the `Random` module.\n",
    "- Once the data is loaded and encoded, generate an index vector by calling the `crossvalidation` function.\n",
    "- Create a function called `trainClassANN`, which receives as parameters the topology, the training set and the indices used for cross-validation. Optionally, it can receive the rest of the parameters used in previous assignments. Inside this function, the following steps may be followed:\n",
    "    - Create a vector with k elements, which will contain the test results of the cross-validation process with the selected metric. If more than one metric is to be used, create one vector per metric.\n",
    "    - Make a loop with k iterations (k folds) where, within each iteration, 4 matrices are created from the desired input and output matrices by means of the index vector resulting from the previous function. Namely, the desired inputs and outputs for training and test. As always, do this process of creating new matrices without loops.\n",
    "    - Within this loop, add a call to generate the model with the training set, and test with the corresponding test set according to the value of k. This can be done by calling the `trainClassANN` function developed in previous assignments, passing as parameters the corresponding sets.\n",
    "    - As indicated in the previous assignment, the training of ANNs is not deterministic, so that, for each iteration k of the cross-validation, it will be necessary to train several ANNs and return the average of the test results (with the selected metric or metrics) in order to have the test value corresponding to this k.\n",
    "    - Furthermore, in the case of training ANNs, the training set can be split into training and validation if the ratio of patterns to be used for the validation set is greater than 0. To do this, use the `holdOut` function developed in a previous assignment.\n",
    "    - Once the model has been trained (several times) on each fold, take the result and fill in the vector(s) created earlier (one for each metric).\n",
    "    - Finally, provide the result of averaging the values of these vectors for each metric together with their standard deviations.\n",
    "    - As a result of this call, at least the test value in the selected metric(s) should be returned. If the model is not deterministic (as is the case for the ANNs), it will be the average of the results of several trainings.\n",
    "- Once this function is done, develop a second one, of the same name, so that it accepts as desired outputs a vector instead of an array, as in a previous assignment, and its operation is simply to make a call to this newly developed function.\n",
    "\n",
    "> **Remarks**:\n",
    "> - Although we have only seen how to train ANNs, in the next assignment we will use other models contained in another library (Scikit-Learn). The idea is to use the same code used for cross-validation with this global loop, changing only the line in which the model is generated.\n",
    "> - Note that other Machine Learning models are deterministic, so they do not need the inner loop (whenever they are trained with the same data they return the same outputs), but only the loop for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44bcc770",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainClassANN (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Statistics\n",
    "function trainClassANN(topology::AbstractArray{<:Int,1}, \n",
    "    trainingDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}}, \n",
    "    kFoldIndices::\tArray{Int64,1}; \n",
    "    transferFunctions::AbstractArray{<:Function,1}=fill(σ, length(topology)), \n",
    "    maxEpochs::Int=1000, minLoss::Real=0.0, learningRate::Real=0.01, repetitionsTraining::Int=1, \n",
    "    validationRatio::Real=0.0, maxEpochsVal::Int=20)\n",
    "    \n",
    "    numFolds = maximum(kFoldIndices); \n",
    "    inputs, targets = trainingDataset;\n",
    "\n",
    "    numMetrics = 7;\n",
    "    metrics = Matrix{Float64}(undef, numFolds, numMetrics);\n",
    "\n",
    "\n",
    "    for numFold in 1:numFolds\n",
    "            \n",
    "        trainingInputs  =       inputs[kFoldIndices.!=  numFold,:]; \n",
    "        testInputs      =       inputs[kFoldIndices.==  numFold,:];\n",
    "        \n",
    "        trainingTargets =       targets[kFoldIndices.!= numFold,:];\n",
    "        testTargets     =       targets[kFoldIndices.== numFold,:];\n",
    "\n",
    "                                \n",
    "      \n",
    "        testAccuraciesEachRepetition = Array{Float64,1}(undef, repetitionsTraining);\n",
    "        testF1EachRepetition = Array{Float64,1}(undef, repetitionsTraining);\n",
    "\n",
    "        metricsFold = Matrix{Float64}(undef, repetitionsTraining, numMetrics);\n",
    "\n",
    "        for numTraining in 1:repetitionsTraining\n",
    "\n",
    "            if validationRatio>0\n",
    "                (trainingIndices, validationIndices) = holdOut(size(trainingInputs,1),\n",
    "                validationRatio*size(trainingInputs,1)/size(inputs,1));\n",
    "\n",
    "                ann, = trainClassANN(topology,\n",
    "                                      (trainingInputs[trainingIndices], trainingTargets[trainingIndices]),\n",
    "                                      validationDataset=(trainingInputs[validationIndices], trainingTargets[validationIndices]),\n",
    "                                      testDataset=(testInputs, testTargets),  \n",
    "                                      transferFunctions=transferFunctions, maxEpochs=maxEpochs, minLoss=minLoss,\n",
    "                                      learningRate=learningRate, maxEpochsVal=maxEpochsVal)\n",
    "                    \n",
    "            else\n",
    "\n",
    "                ann, = trainClassANN(topology, (trainingInputs, trainingTargets),\n",
    "                                       testDataset=(testInputs, testTargets), \n",
    "                                       transferFunctions=transferFunctions, maxEpochs=maxEpochs, minLoss=minLoss,\n",
    "                                       learningRate=learningRate, maxEpochsVal=maxEpochsVal)\n",
    "            end;\n",
    "\n",
    "            metricsIter = collect(confusionMatrix(ann(testInputs')', testTargets)[1:7])\n",
    "            metricsFold[numTraining, :] .= metricsIter      \n",
    "        end;\n",
    "        metrics[numFold, :] .= mean(metricsFold, dims=1)[1, :]\n",
    "    end;\n",
    "    metricsAvg = mean(metrics, dims=1)\n",
    "    metricsStd = std(metrics, dims=1)\n",
    "    return (metricsAvg[1, :], metricsStd[1, :])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64786f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>149×5 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">124 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">5.1</th><th style = \"text-align: left;\">3.5</th><th style = \"text-align: left;\">1.4</th><th style = \"text-align: left;\">0.2</th><th style = \"text-align: left;\">Iris-setosa</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"String15\" style = \"text-align: left;\">String15</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">4.9</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">4.7</td><td style = \"text-align: right;\">3.2</td><td style = \"text-align: right;\">1.3</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">4.6</td><td style = \"text-align: right;\">3.1</td><td style = \"text-align: right;\">1.5</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: right;\">5.0</td><td style = \"text-align: right;\">3.6</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: right;\">5.4</td><td style = \"text-align: right;\">3.9</td><td style = \"text-align: right;\">1.7</td><td style = \"text-align: right;\">0.4</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: right;\">4.6</td><td style = \"text-align: right;\">3.4</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.3</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: right;\">5.0</td><td style = \"text-align: right;\">3.4</td><td style = \"text-align: right;\">1.5</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: right;\">4.4</td><td style = \"text-align: right;\">2.9</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: right;\">4.9</td><td style = \"text-align: right;\">3.1</td><td style = \"text-align: right;\">1.5</td><td style = \"text-align: right;\">0.1</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: right;\">5.4</td><td style = \"text-align: right;\">3.7</td><td style = \"text-align: right;\">1.5</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: right;\">4.8</td><td style = \"text-align: right;\">3.4</td><td style = \"text-align: right;\">1.6</td><td style = \"text-align: right;\">0.2</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: right;\">4.8</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">1.4</td><td style = \"text-align: right;\">0.1</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: right;\">4.3</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">1.1</td><td style = \"text-align: right;\">0.1</td><td style = \"text-align: left;\">Iris-setosa</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">138</td><td style = \"text-align: right;\">6.0</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">4.8</td><td style = \"text-align: right;\">1.8</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">139</td><td style = \"text-align: right;\">6.9</td><td style = \"text-align: right;\">3.1</td><td style = \"text-align: right;\">5.4</td><td style = \"text-align: right;\">2.1</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">140</td><td style = \"text-align: right;\">6.7</td><td style = \"text-align: right;\">3.1</td><td style = \"text-align: right;\">5.6</td><td style = \"text-align: right;\">2.4</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">141</td><td style = \"text-align: right;\">6.9</td><td style = \"text-align: right;\">3.1</td><td style = \"text-align: right;\">5.1</td><td style = \"text-align: right;\">2.3</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">142</td><td style = \"text-align: right;\">5.8</td><td style = \"text-align: right;\">2.7</td><td style = \"text-align: right;\">5.1</td><td style = \"text-align: right;\">1.9</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">143</td><td style = \"text-align: right;\">6.8</td><td style = \"text-align: right;\">3.2</td><td style = \"text-align: right;\">5.9</td><td style = \"text-align: right;\">2.3</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">144</td><td style = \"text-align: right;\">6.7</td><td style = \"text-align: right;\">3.3</td><td style = \"text-align: right;\">5.7</td><td style = \"text-align: right;\">2.5</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">145</td><td style = \"text-align: right;\">6.7</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">5.2</td><td style = \"text-align: right;\">2.3</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">146</td><td style = \"text-align: right;\">6.3</td><td style = \"text-align: right;\">2.5</td><td style = \"text-align: right;\">5.0</td><td style = \"text-align: right;\">1.9</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">147</td><td style = \"text-align: right;\">6.5</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">5.2</td><td style = \"text-align: right;\">2.0</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">148</td><td style = \"text-align: right;\">6.2</td><td style = \"text-align: right;\">3.4</td><td style = \"text-align: right;\">5.4</td><td style = \"text-align: right;\">2.3</td><td style = \"text-align: left;\">Iris-virginica</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">149</td><td style = \"text-align: right;\">5.9</td><td style = \"text-align: right;\">3.0</td><td style = \"text-align: right;\">5.1</td><td style = \"text-align: right;\">1.8</td><td style = \"text-align: left;\">Iris-virginica</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& 5.1 & 3.5 & 1.4 & 0.2 & Iris-setosa\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & String15\\\\\n",
       "\t\\hline\n",
       "\t1 & 4.9 & 3.0 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t2 & 4.7 & 3.2 & 1.3 & 0.2 & Iris-setosa \\\\\n",
       "\t3 & 4.6 & 3.1 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\t4 & 5.0 & 3.6 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t5 & 5.4 & 3.9 & 1.7 & 0.4 & Iris-setosa \\\\\n",
       "\t6 & 4.6 & 3.4 & 1.4 & 0.3 & Iris-setosa \\\\\n",
       "\t7 & 5.0 & 3.4 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\t8 & 4.4 & 2.9 & 1.4 & 0.2 & Iris-setosa \\\\\n",
       "\t9 & 4.9 & 3.1 & 1.5 & 0.1 & Iris-setosa \\\\\n",
       "\t10 & 5.4 & 3.7 & 1.5 & 0.2 & Iris-setosa \\\\\n",
       "\t11 & 4.8 & 3.4 & 1.6 & 0.2 & Iris-setosa \\\\\n",
       "\t12 & 4.8 & 3.0 & 1.4 & 0.1 & Iris-setosa \\\\\n",
       "\t13 & 4.3 & 3.0 & 1.1 & 0.1 & Iris-setosa \\\\\n",
       "\t14 & 5.8 & 4.0 & 1.2 & 0.2 & Iris-setosa \\\\\n",
       "\t15 & 5.7 & 4.4 & 1.5 & 0.4 & Iris-setosa \\\\\n",
       "\t16 & 5.4 & 3.9 & 1.3 & 0.4 & Iris-setosa \\\\\n",
       "\t17 & 5.1 & 3.5 & 1.4 & 0.3 & Iris-setosa \\\\\n",
       "\t18 & 5.7 & 3.8 & 1.7 & 0.3 & Iris-setosa \\\\\n",
       "\t19 & 5.1 & 3.8 & 1.5 & 0.3 & Iris-setosa \\\\\n",
       "\t20 & 5.4 & 3.4 & 1.7 & 0.2 & Iris-setosa \\\\\n",
       "\t21 & 5.1 & 3.7 & 1.5 & 0.4 & Iris-setosa \\\\\n",
       "\t22 & 4.6 & 3.6 & 1.0 & 0.2 & Iris-setosa \\\\\n",
       "\t23 & 5.1 & 3.3 & 1.7 & 0.5 & Iris-setosa \\\\\n",
       "\t24 & 4.8 & 3.4 & 1.9 & 0.2 & Iris-setosa \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m149×5 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m 5.1     \u001b[0m\u001b[1m 3.5     \u001b[0m\u001b[1m 1.4     \u001b[0m\u001b[1m 0.2     \u001b[0m\u001b[1m Iris-setosa    \u001b[0m\n",
       "     │\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m String15       \u001b[0m\n",
       "─────┼────────────────────────────────────────────────────\n",
       "   1 │     4.9      3.0      1.4      0.2  Iris-setosa\n",
       "   2 │     4.7      3.2      1.3      0.2  Iris-setosa\n",
       "   3 │     4.6      3.1      1.5      0.2  Iris-setosa\n",
       "   4 │     5.0      3.6      1.4      0.2  Iris-setosa\n",
       "   5 │     5.4      3.9      1.7      0.4  Iris-setosa\n",
       "   6 │     4.6      3.4      1.4      0.3  Iris-setosa\n",
       "   7 │     5.0      3.4      1.5      0.2  Iris-setosa\n",
       "   8 │     4.4      2.9      1.4      0.2  Iris-setosa\n",
       "  ⋮  │    ⋮        ⋮        ⋮        ⋮           ⋮\n",
       " 143 │     6.8      3.2      5.9      2.3  Iris-virginica\n",
       " 144 │     6.7      3.3      5.7      2.5  Iris-virginica\n",
       " 145 │     6.7      3.0      5.2      2.3  Iris-virginica\n",
       " 146 │     6.3      2.5      5.0      1.9  Iris-virginica\n",
       " 147 │     6.5      3.0      5.2      2.0  Iris-virginica\n",
       " 148 │     6.2      3.4      5.4      2.3  Iris-virginica\n",
       " 149 │     5.9      3.0      5.1      1.8  Iris-virginica\n",
       "\u001b[36m                                          134 rows omitted\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "using Random\n",
    "using Statistics\n",
    "\n",
    "iris_df = CSV.read(\"./data/iris/iris.data\", DataFrame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad587be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainClassANN (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "include(\"./functionsLibrary.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1861fcc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149×3 Matrix{Bool}:\n",
       " 1  0  0\n",
       " 1  0  0\n",
       " 1  0  0\n",
       " 1  0  0\n",
       " 1  0  0\n",
       " 1  0  0\n",
       " 1  0  0\n",
       " 1  0  0\n",
       " 1  0  0\n",
       " 1  0  0\n",
       " ⋮     \n",
       " 0  0  1\n",
       " 0  0  1\n",
       " 0  0  1\n",
       " 0  0  1\n",
       " 0  0  1\n",
       " 0  0  1\n",
       " 0  0  1\n",
       " 0  0  1\n",
       " 0  0  1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "inputs = Matrix(iris_df[:, 1:4])  # Take first 4 columns as input features\n",
    "labels = iris_df[:, :5]     # Take the Species column as the target\n",
    "\n",
    "# One-hot encoding of labels (Iris Setosa, Versicolour, and Virginica)\n",
    "classes = unique(labels)\n",
    "targets = oneHotEncoding(labels, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d871940b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define topology: 4 input features -> 5 hidden neurons -> 3 output classes\n",
    "topology = [4, 5]\n",
    "\n",
    "# Define other parameters\n",
    "learning_rate = 0.01\n",
    "max_epochs = 100\n",
    "repetitions_training = 3\n",
    "validation_ratio = 0.2  # 20% of training data used for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e7e043d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Manifest.toml`\n",
      "┌ Warning: Layer with Float32 parameters got Float64 input.\n",
      "│   The input will be converted, but any earlier layers may be very slow.\n",
      "│   layer = Dense(4 => 4, σ)\n",
      "│   summary(x) = 4×134 adjoint(::Matrix{Float64}) with eltype Float64\n",
      "└ @ Flux /home/djove/.julia/packages/Flux/htpCe/src/layers/stateless.jl:59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop criteria:\n",
      "numEpoch 1000<maxEpochs 1000\n",
      "trainingLoss 0.09289962>minLoss 0.0\n",
      "numEpochsValidation 1<maxEpochsVal 20\n",
      "Stop criteria:\n",
      "numEpoch 1000<maxEpochs 1000\n",
      "trainingLoss 0.10137406>minLoss 0.0\n",
      "numEpochsValidation 1<maxEpochsVal 20\n",
      "Stop criteria:\n",
      "numEpoch 1000<maxEpochs 1000\n",
      "trainingLoss 0.07237235>minLoss 0.0\n",
      "numEpochsValidation 1<maxEpochsVal 20\n",
      "Stop criteria:\n",
      "numEpoch 1000<maxEpochs 1000\n",
      "trainingLoss 0.11441504>minLoss 0.0\n",
      "numEpochsValidation 1<maxEpochsVal 20\n",
      "Stop criteria:\n",
      "numEpoch 1000<maxEpochs 1000\n",
      "trainingLoss 0.08565611>minLoss 0.0\n",
      "numEpochsValidation 1<maxEpochsVal 20\n",
      "Stop criteria:\n",
      "numEpoch 1000<maxEpochs 1000\n",
      "trainingLoss 0.08221638>minLoss 0.0\n",
      "numEpochsValidation 1<maxEpochsVal 20\n",
      "Stop criteria:\n",
      "numEpoch 1000<maxEpochs 1000\n",
      "trainingLoss 0.07716699>minLoss 0.0\n",
      "numEpochsValidation 1<maxEpochsVal 20\n",
      "Stop criteria:\n",
      "numEpoch 1000<maxEpochs 1000\n",
      "trainingLoss 0.07182079>minLoss 0.0\n",
      "numEpochsValidation 1<maxEpochsVal 20\n",
      "Stop criteria:\n",
      "numEpoch 1000<maxEpochs 1000\n",
      "trainingLoss 0.0969012>minLoss 0.0\n",
      "numEpochsValidation 1<maxEpochsVal 20\n",
      "Stop criteria:\n",
      "numEpoch 1000<maxEpochs 1000\n",
      "trainingLoss 0.0759994>minLoss 0.0\n",
      "numEpochsValidation 1<maxEpochsVal 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.9595238095238097, 0.040476190476190464, 1.0, 1.0, 1.0, 1.0, 1.0], [0.046939597087686476, 0.046939597087686476, 0.0, 0.0, 0.0, 0.0, 0.0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Pkg.add(\"Flux\")\n",
    "using Flux\n",
    "\n",
    "(metricsAvg, metricsStd) = trainClassANN(topology, (inputs, targets), crossvalidation(targets,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8272e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average metrics across folds: [0.9595238095238097, 0.040476190476190464, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Standard deviation of metrics across folds: [0.046939597087686476, 0.046939597087686476, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "println(\"Average metrics across folds: \", metricsAvg)\n",
    "println(\"Standard deviation of metrics across folds: \", metricsStd)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
